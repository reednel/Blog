---
layout: post
author: Reed Nelson
title: "Artificial Consciousness and Phenomenology: an Underbaked Theory"
---

### Intro
Perceptions of what it takes for a computer to have consciousness seem to vary radically from person to person. If you have lower standards, perhaps you think it possible that the internet, or even just individual computers already have some level of consciousness. At the other extreme, you could think real artificial consciousness is centuries off or even impossible (and you’d be in the company of [some experts](https://www.researchgate.net/publication/280838978_Future_Progress_in_Artificial_Intelligence_A_Survey_of_Expert_Opinion)!). I won’t bother describing where I think the average person falls, or where you fall, but I will say I probably think your standards are too low, and that’s why I decided to write about this topic. My goal in this post is to make you consider aspects consciousness you might not have before, and perhaps give you cause to update your intuition on this matter.

To get us on the same page and in the right headspace, I’ll define a few terms before moving forward. Artificial narrow intelligences (ANIs) are systems capable of doing a narrow range of tasks, usually as well or better than a human could. This describes all present-day AIs (think Deep Blue, AlphaZero, GPT-3, Alexa…).  Artificial General Intelligence (AGI) refers to a system that can do most everything an average human can, at least in feats of intellect. Consciousness can be a tricky thing to talk and think about. We’ll define it as Google does: “the fact of awareness by the mind of itself and the world”.  

Notice that this definition of AGI says absolutely nothing about consciousness! If you can agree that the bits moving through caches to tell you an optimal chess move do not constitute consciousness, then you should also agree that the concatenation of this functionality with a million other individual human-like functions still doesn’t sum to consciousness. And this probably goes without saying, but don’t be fooled by sophisticated chat bots or computer vision algorithms, what’s going on under the hood is fundamentally the same, with respect to our concerns in this discussion. That is to say, you can theoretically have AGI without consciousness. 

At this point you might be asking yourself “but Reed, it can do things! What does it matter whether it has consciousness?”. I leave this question as an exercise to the reader. 

Now for the fun part: dated and out-of-fashion philosophy. This tradition was started by Edmund Husserl and was present in the works of a number of notable European philosophers of the early-mid 20th century. Phenomenology is essentially (if you can believe it) the study of phenomena. Phenomenology puts first the subjective first-personal experience, the what-it’s-like-ness of a phenomenon, and considers second the aspect of a phenomenon that one thinks about, that one can describe with words and conventionally analyze. I think a phenomenological approach to the question of what it takes to create consciousness is important, because 



### Additional Reading

#### Alan Turing - [Computing Machinery and Intelligence](https://academic.oup.com/mind/article/LIX/236/433/986238) (1950) 

This is one of Turing’s most famous papers. This is where the [Turing Test](https://en.wikipedia.org/wiki/Turing_test) is debuted, and its among the earliest works in this domain. And it’s remarkably easy to read for its age! 

#### John Searle - [Minds, Brains, and Programs](chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/viewer.html?pdfurl=https%3A%2F%2Fwww.law.upenn.edu%2Flive%2Ffiles%2F3413-searle-j-minds-brains-and-programs-1980pdf&chunk=true) (1980) 

Like Turing’s, this work is also famous for its thought experiment: the [Chinese Room](https://en.wikipedia.org/wiki/Chinese_room). The Chinese Room is meant to show that a machine’s ability to pass the Turing test is no measure of its ability to think, in a human-ish sense. I find the argument pretty convincing, and I suspect most do.

Fun fact: Searle did undergrad at UW!

#### Hubert Dreyfus - [What Computers (Still) Can’t Do](https://www.penguin.com.au/books/what-computers-still-cant-do-9780262540674) (1972/1992) 

This is a whole book. I can’t say whether it’s all worth reading, but there are certainly some interesting takeaways if you can find a summary. Hubert and his brother Stuart developed a [model of skill acquisition](https://www.kaizenko.com/the-dreyfus-model-of-skills-acquisition/), with which they argue that machines could never develop the skills humans do, in the depth that we do them. This is because while computers operate solely on explicit instruction, skills that humans excel at are executed with a high degree of “tacit knowledge” that cannot be reduced to explicit instructions (think muscle memory, kind of). However, many significant milestones have been passed since this book’s publication, and there now exists AI which can handle tacit knowledge pretty well. 

#### Ragnar Fjelland - [Why General Artificial Intelligence will not be Realized](https://www.nature.com/articles/s41599-020-0494-4) (2020) 

Fjelland’s goal in this paper is to show that Dreyfus’s argument still holds, despite the extensive development of artificial neural networks. He argues that modern methods (e.g. ML) are merely correlative, they know nothing of causation. A higher level of reasoning, with a deeper understanding of the events in question and the context they occured in is necessary to determine causation. Thus, a machine that is not in-the-world cannot reason about causal relationships. 

>“We are bodily and social beings, living in a material and social world. To understand another person is not to look into the chemistry of that person’s brain, not even into that person’s soul, but is rather to be in that person’s ‘shoes’. It is to understand that person’s lifeworld.” 
